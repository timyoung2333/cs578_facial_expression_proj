{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "from FER2013 import FER2013\n",
    "from Perceptron import Perceptron\n",
    "from Adaboost import AdaBoost\n",
    "from MLP import MLP\n",
    "from SVM import SVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "\n",
    "    def __init__(self, raw_data, subset_size): # You may change the directory to make the code work\n",
    "        \"\"\"\n",
    "        Load data\n",
    "        \"\"\"\n",
    "        self.fer = raw_data\n",
    "        self.subDataset = self.fer.getSubDataset(subset_size)\n",
    "        self.num = subset_size\n",
    "        self.label2expression = {\n",
    "            0: \"Angry\",\n",
    "            1: \"Disgust\",\n",
    "            2: \"Fear\",\n",
    "            3: \"Happy\",\n",
    "            4: \"Sad\",\n",
    "            5: \"Surprise\",\n",
    "            6: \"Neutral\"\n",
    "        }\n",
    "    \n",
    "    def getFeatureSize(self, mode):\n",
    "        if mode == \"raw\":\n",
    "            return 2304\n",
    "    \n",
    "    \n",
    "    def kfoldSplit(self, k, idx):\n",
    "        \"\"\"\n",
    "        idx represents the index of the fold of test set. \n",
    "        \"\"\"\n",
    "        feature_size = self.getFeatureSize(\"raw\")\n",
    "        test_size = int(self.num / k)\n",
    "        train_size = self.num - test_size\n",
    "        test_id = np.arange(int(idx*self.num/k), int((idx+1)*self.num/k))\n",
    "        train_id = np.concatenate((np.arange(0, int(idx*self.num/k)), np.arange(int((idx+1)*self.num/k), self.num)))\n",
    "        temp_train_data, temp_test_data = np.empty((0,feature_size+1)), np.empty((0,feature_size+1))\n",
    "        for i in range(7):\n",
    "            temp_train_data = np.append(temp_train_data, np.concatenate((self.subDataset[i][train_id], np.array([[i] * train_size]).T), axis=1), axis=0)\n",
    "            temp_test_data = np.append(temp_test_data, np.concatenate((self.subDataset[i][test_id], np.array([[i] * test_size]).T), axis=1), axis=0)\n",
    "\n",
    "        # Previously, the data types of both training data and test data are [0,0,0,...,1,1,1,...,2,2,2,...]\n",
    "        # Now we shuffle them to ensure a random order.\n",
    "        np.random.shuffle(temp_train_data)\n",
    "        np.random.shuffle(temp_test_data)\n",
    "\n",
    "        X_train, y_train = temp_train_data[:, 0:feature_size], temp_train_data[:, feature_size]\n",
    "        X_test, y_test = temp_test_data[:, 0:feature_size], temp_test_data[:, feature_size]\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def testModel(self, k, model):\n",
    "        X_train, y_train, X_test, y_test = self.kfoldSplit(k, 0)\n",
    "        model.train(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(model.score(X_test, y_test))\n",
    "    \n",
    "    def kfoldCrossValidation(self, k, model):\n",
    "        train_acc, test_acc = 0, 0\n",
    "        cf_matrix = np.zeros((7, 7)) # y-axis is true label, x-axis is predicted label\n",
    "        for t in range(0, k):\n",
    "            X_train, y_train, X_test, y_test = self.kfoldSplit(k, t)\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            train_acc += model.score(X_train, y_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            test_acc += model.score(X_test, y_test)\n",
    "            cf_matrix += confusion_matrix(y_test, y_pred_test)\n",
    "        train_acc, test_acc = train_acc / k, test_acc / k\n",
    "        for em in range(0, 7):\n",
    "            cf_matrix[em] = np.divide(cf_matrix[em], np.sum(cf_matrix[em]))\n",
    "        return train_acc, test_acc, cf_matrix\n",
    "        \n",
    "    # def bootstrappingSplit(self):\n",
    "        \n",
    "\n",
    "    # def bootstrapping(self, B):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FER2013 dataset from /Users/timyang/Downloads/CS578-Project-master/data/icml_face_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35887/35887 [00:20<00:00, 1770.29it/s]\n"
     ]
    }
   ],
   "source": [
    "fer = FER2013(filename=\"/Users/timyang/Downloads/CS578-Project-master/data/icml_face_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1-- Epoch 1\n",
      "-- Epoch 1\n",
      "\n",
      "Norm: 175.18, NNZs: 2304, Bias: 14.000000, T: 3150, Avg. loss: 85.438638\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 166.86, NNZs: 2304, Bias: -12.000000, T: 3150, Avg. loss: 91.657092\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 205.69, NNZs: 2304, Bias: 1.000000, T: 3150, Avg. loss: 78.746806\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 191.41, NNZs: 2304, Bias: -12.000000, T: 3150, Avg. loss: 85.058644-- Epoch 2\n",
      "\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 163.81, NNZs: 2304, Bias: 7.000000, T: 3150, Avg. loss: 82.305207\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 213.77, NNZs: 2304, Bias: -61.000000, T: 3150, Avg. loss: 88.582114\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 179.43, NNZs: 2304, Bias: 18.000000, T: 3150, Avg. loss: 81.216620\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 274.21, NNZs: 2304, Bias: 23.000000, T: 6300, Avg. loss: 80.144127\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 249.79, NNZs: 2304, Bias: -28.000000, T: 6300, Avg. loss: 84.614119\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 295.68, NNZs: 2304, Bias: -8.000000, T: 6300, Avg. loss: 74.206904Norm: 296.41, NNZs: 2304, Bias: -35.000000, T: 6300, Avg. loss: 80.109314\n",
      "Total training time: 0.04 seconds.\n",
      "\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "-- Epoch 3\n",
      "Norm: 233.13, NNZs: 2304, Bias: 20.000000, T: 6300, Avg. loss: 79.497659\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 321.55, NNZs: 2304, Bias: -101.000000, T: 6300, Avg. loss: 81.708047\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 260.58, NNZs: 2304, Bias: 35.000000, T: 6300, Avg. loss: 77.963462\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 346.14, NNZs: 2304, Bias: 28.000000, T: 9450, Avg. loss: 80.542041\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 323.65, NNZs: 2304, Bias: -45.000000, T: 9450, Avg. loss: 86.624143\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 368.71, NNZs: 2304, Bias: -41.000000, T: 9450, Avg. loss: 76.918517Norm: 374.55, NNZs: 2304, Bias: -15.000000, T: 9450, Avg. loss: 69.402101\n",
      "Total training time: 0.05 seconds.\n",
      "\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "-- Epoch 4\n",
      "Norm: 299.34, NNZs: 2304, Bias: 28.000000, T: 9450, Avg. loss: 77.902992\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 401.75, NNZs: 2304, Bias: -146.000000, T: 9450, Avg. loss: 78.311308\n",
      "Total training time: 0.05 seconds.\n",
      "Norm: 330.07, NNZs: 2304, Bias: 56.000000, T: 9450, Avg. loss: 73.593824-- Epoch 4\n",
      "\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 402.70, NNZs: 2304, Bias: 28.000000, T: 12600, Avg. loss: 75.949571\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 378.43, NNZs: 2304, Bias: -50.000000, T: 12600, Avg. loss: 83.578272\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 428.71, NNZs: 2304, Bias: -25.000000, T: 12600, Avg. loss: 68.783876\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 447.27, NNZs: 2304, Bias: -58.000000, T: 12600, Avg. loss: 78.129069\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 358.57, NNZs: 2304, Bias: 32.000000, T: 12600, Avg. loss: 70.154265\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 386.41, NNZs: 2304, Bias: 69.000000, T: 12600, Avg. loss: 70.412242\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 461.28, NNZs: 2304, Bias: -178.000000, T: 12600, Avg. loss: 71.548393\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 466.29, NNZs: 2304, Bias: 29.000000, T: 15750, Avg. loss: 74.715358\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 431.14, NNZs: 2304, Bias: -58.000000, T: 15750, Avg. loss: 81.718235\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 472.34, NNZs: 2304, Bias: -29.000000, T: 15750, Avg. loss: 69.518940\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 514.97, NNZs: 2304, Bias: -64.000000, T: 15750, Avg. loss: 73.471437\n",
      "Total training time: 0.09 seconds.\n",
      "Norm: 414.69, NNZs: 2304, Bias: 37.000000, T: 15750, Avg. loss: 70.724952-- Epoch 6\n",
      "\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 439.61, NNZs: 2304, Bias: 80.000000, T: 15750, Avg. loss: 67.275835\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 520.29, NNZs: 2304, Bias: -212.000000, T: 15750, Avg. loss: 73.082534\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 518.77, NNZs: 2304, Bias: 28.000000, T: 18900, Avg. loss: 71.177838\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 486.02, NNZs: 2304, Bias: -76.000000, T: 18900, Avg. loss: 79.596680\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 529.87, NNZs: 2304, Bias: -39.000000, T: 18900, Avg. loss: 65.741801\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 572.55, NNZs: 2304, Bias: -77.000000, T: 18900, Avg. loss: 69.740849\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7Norm: 463.11, NNZs: 2304, Bias: 37.000000, T: 18900, Avg. loss: 73.812610\n",
      "\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 491.10, NNZs: 2304, Bias: 83.000000, T: 18900, Avg. loss: 68.779827\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 565.69, NNZs: 2304, Bias: -238.000000, T: 18900, Avg. loss: 73.356854\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 583.25, NNZs: 2304, Bias: -41.000000, T: 22050, Avg. loss: 64.346322Norm: 503.40, NNZs: 2304, Bias: 42.000000, T: 22050, Avg. loss: 69.976457\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 8\n",
      "\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 534.19, NNZs: 2304, Bias: -78.000000, T: 22050, Avg. loss: 78.951102Norm: 535.17, NNZs: 2304, Bias: 95.000000, T: 22050, Avg. loss: 69.158992\n",
      "Total training time: 0.12 seconds.\n",
      "\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 634.74, NNZs: 2304, Bias: -90.000000, T: 22050, Avg. loss: 72.891454Norm: 573.75, NNZs: 2304, Bias: 32.000000, T: 22050, Avg. loss: 70.489528-- Epoch 8\n",
      "\n",
      "Total training time: 0.12 seconds.\n",
      "\n",
      "Total training time: 0.12 seconds.\n",
      "Norm: 622.58, NNZs: 2304, Bias: -270.000000, T: 22050, Avg. loss: 71.889729-- Epoch 8\n",
      "-- Epoch 8\n",
      "\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 633.91, NNZs: 2304, Bias: -42.000000, T: 25200, Avg. loss: 68.729285\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 549.04, NNZs: 2304, Bias: 50.000000, T: 25200, Avg. loss: 67.995911\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 585.41, NNZs: 2304, Bias: -83.000000, T: 25200, Avg. loss: 80.378997\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 581.49, NNZs: 2304, Bias: 110.000000, T: 25200, Avg. loss: 64.587176\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 684.69, NNZs: 2304, Bias: -290.000000, T: 25200, Avg. loss: 68.413203\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 631.47, NNZs: 2304, Bias: 34.000000, T: 25200, Avg. loss: 70.066655Norm: 689.30, NNZs: 2304, Bias: -99.000000, T: 25200, Avg. loss: 67.968545\n",
      "Total training time: 0.14 seconds.\n",
      "\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 9\n",
      "-- Epoch 9\n",
      "Norm: 666.69, NNZs: 2304, Bias: -43.000000, T: 28350, Avg. loss: 64.091153\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 592.66, NNZs: 2304, Bias: 53.000000, T: 28350, Avg. loss: 67.826270\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 640.17, NNZs: 2304, Bias: -90.000000, T: 28350, Avg. loss: 76.637574\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 620.44, NNZs: 2304, Bias: 118.000000, T: 28350, Avg. loss: 65.081283\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 727.72, NNZs: 2304, Bias: -316.000000, T: 28350, Avg. loss: 67.230477\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 735.90, NNZs: 2304, Bias: -100.000000, T: 28350, Avg. loss: 65.416685\n",
      "Total training time: 0.16 seconds.Norm: 676.90, NNZs: 2304, Bias: 36.000000, T: 28350, Avg. loss: 69.454931\n",
      "\n",
      "-- Epoch 10\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 713.41, NNZs: 2304, Bias: -48.000000, T: 31500, Avg. loss: 62.684017\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 630.26, NNZs: 2304, Bias: 51.000000, T: 31500, Avg. loss: 70.055664\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 687.71, NNZs: 2304, Bias: -97.000000, T: 31500, Avg. loss: 75.310587\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 659.52, NNZs: 2304, Bias: 123.000000, T: 31500, Avg. loss: 64.742861\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 769.90, NNZs: 2304, Bias: -339.000000, T: 31500, Avg. loss: 62.933073\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 789.15, NNZs: 2304, Bias: -106.000000, T: 31500, Avg. loss: 65.729155\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 723.76, NNZs: 2304, Bias: 36.000000, T: 31500, Avg. loss: 64.868396\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 756.46, NNZs: 2304, Bias: -59.000000, T: 34650, Avg. loss: 60.007842\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 665.11, NNZs: 2304, Bias: 59.000000, T: 34650, Avg. loss: 69.798745\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 729.48, NNZs: 2304, Bias: -98.000000, T: 34650, Avg. loss: 78.516037\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 810.81, NNZs: 2304, Bias: -357.000000, T: 34650, Avg. loss: 63.963097\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 703.79, NNZs: 2304, Bias: 123.000000, T: 34650, Avg. loss: 66.596872\n",
      "Total training time: 0.19 seconds.\n",
      "Norm: 838.46, NNZs: 2304, Bias: -106.000000, T: 34650, Avg. loss: 62.562948\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "-- Epoch 12\n",
      "Norm: 761.57, NNZs: 2304, Bias: 37.000000, T: 34650, Avg. loss: 65.805000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 765.80, NNZs: 2304, Bias: -108.000000, T: 37800, Avg. loss: 75.411236Norm: 797.25, NNZs: 2304, Bias: -68.000000, T: 37800, Avg. loss: 57.695182Norm: 877.92, NNZs: 2304, Bias: -120.000000, T: 37800, Avg. loss: 59.909893\n",
      "Total training time: 0.21 seconds.\n",
      "\n",
      "Total training time: 0.21 seconds.-- Epoch 13\n",
      "Norm: 857.85, NNZs: 2304, Bias: -384.000000, T: 37800, Avg. loss: 60.247332\n",
      "\n",
      "-- Epoch 13Norm: 707.95, NNZs: 2304, Bias: 65.000000, T: 37800, Avg. loss: 63.683613\n",
      "Total training time: 0.21 seconds.\n",
      "Total training time: 0.21 seconds.\n",
      "\n",
      "-- Epoch 13\n",
      "\n",
      "Norm: 742.44, NNZs: 2304, Bias: 114.000000, T: 37800, Avg. loss: 62.067437\n",
      "-- Epoch 13Total training time: 0.21 seconds.\n",
      "\n",
      "Norm: 808.33, NNZs: 2304, Bias: 36.000000, T: 37800, Avg. loss: 64.050505Total training time: 0.21 seconds.-- Epoch 13\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 13\n",
      "\n",
      "\n",
      "-- Epoch 13\n",
      "Norm: 836.99, NNZs: 2304, Bias: -77.000000, T: 40950, Avg. loss: 59.030948\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 750.42, NNZs: 2304, Bias: 58.000000, T: 40950, Avg. loss: 65.998784Norm: 917.88, NNZs: 2304, Bias: -130.000000, T: 40950, Avg. loss: 63.002698\n",
      "Total training time: 0.23 seconds.\n",
      "\n",
      "Total training time: 0.23 seconds.Norm: 904.36, NNZs: 2304, Bias: -402.000000, T: 40950, Avg. loss: 61.817242-- Epoch 14\n",
      "\n",
      "\n",
      "Total training time: 0.23 seconds.\n",
      "Norm: 848.57, NNZs: 2304, Bias: 40.000000, T: 40950, Avg. loss: 66.355314-- Epoch 14\n",
      "-- Epoch 14\n",
      "\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 783.85, NNZs: 2304, Bias: 117.000000, T: 40950, Avg. loss: 63.203824\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 800.39, NNZs: 2304, Bias: -111.000000, T: 40950, Avg. loss: 72.125607\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 872.98, NNZs: 2304, Bias: -79.000000, T: 44100, Avg. loss: 58.360368\n",
      "Total training time: 0.25 seconds.\n",
      "Norm: 960.90, NNZs: 2304, Bias: -135.000000, T: 44100, Avg. loss: 60.235214Norm: 796.03, NNZs: 2304, Bias: 56.000000, T: 44100, Avg. loss: 65.368878\n",
      "Total training time: 0.25 seconds.\n",
      "\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 15\n",
      "-- Epoch 15\n",
      "-- Epoch 15\n",
      "Norm: 949.30, NNZs: 2304, Bias: -426.000000, T: 44100, Avg. loss: 64.640045\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 816.65, NNZs: 2304, Bias: 115.000000, T: 44100, Avg. loss: 60.703638\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 892.15, NNZs: 2304, Bias: 45.000000, T: 44100, Avg. loss: 64.745589\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 843.60, NNZs: 2304, Bias: -124.000000, T: 44100, Avg. loss: 70.738001\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 996.35, NNZs: 2304, Bias: -140.000000, T: 47250, Avg. loss: 63.142973Norm: 906.03, NNZs: 2304, Bias: -82.000000, T: 47250, Avg. loss: 59.987258\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 16\n",
      "\n",
      "Total training time: 0.27 seconds.\n",
      "Norm: 832.33, NNZs: 2304, Bias: 62.000000, T: 47250, Avg. loss: 62.553524-- Epoch 16\n",
      "\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 995.46, NNZs: 2304, Bias: -440.000000, T: 47250, Avg. loss: 60.397203\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 854.64, NNZs: 2304, Bias: 119.000000, T: 47250, Avg. loss: 61.406815\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 940.73, NNZs: 2304, Bias: 41.000000, T: 47250, Avg. loss: 62.508403\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 885.37, NNZs: 2304, Bias: -129.000000, T: 47250, Avg. loss: 71.591714\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 873.36, NNZs: 2304, Bias: 60.000000, T: 50400, Avg. loss: 65.470497\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1029.07, NNZs: 2304, Bias: -143.000000, T: 50400, Avg. loss: 60.458307\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 941.23, NNZs: 2304, Bias: -82.000000, T: 50400, Avg. loss: 57.876146\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1029.44, NNZs: 2304, Bias: -458.000000, T: 50400, Avg. loss: 60.452820\n",
      "Norm: 895.90, NNZs: 2304, Bias: 115.000000, T: 50400, Avg. loss: 59.760502\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 17\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 976.26, NNZs: 2304, Bias: 50.000000, T: 50400, Avg. loss: 64.659903\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 922.47, NNZs: 2304, Bias: -127.000000, T: 50400, Avg. loss: 70.585856\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1064.19, NNZs: 2304, Bias: -150.000000, T: 53550, Avg. loss: 57.870394\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 910.68, NNZs: 2304, Bias: 60.000000, T: 53550, Avg. loss: 61.037617\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 930.06, NNZs: 2304, Bias: 128.000000, T: 53550, Avg. loss: 59.111883\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 981.32, NNZs: 2304, Bias: -81.000000, T: 53550, Avg. loss: 57.265407\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1066.81, NNZs: 2304, Bias: -475.000000, T: 53550, Avg. loss: 58.586020\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1014.79, NNZs: 2304, Bias: 45.000000, T: 53550, Avg. loss: 61.934128\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 962.24, NNZs: 2304, Bias: -127.000000, T: 53550, Avg. loss: 69.912933\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 947.98, NNZs: 2304, Bias: 61.000000, T: 56700, Avg. loss: 61.302435\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1100.33, NNZs: 2304, Bias: -155.000000, T: 56700, Avg. loss: 59.227539\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 961.15, NNZs: 2304, Bias: 129.000000, T: 56700, Avg. loss: 60.542819\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1016.54, NNZs: 2304, Bias: -82.000000, T: 56700, Avg. loss: 57.326504\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1102.43, NNZs: 2304, Bias: -488.000000, T: 56700, Avg. loss: 58.962089\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1047.24, NNZs: 2304, Bias: 43.000000, T: 56700, Avg. loss: 60.977725\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 997.32, NNZs: 2304, Bias: -135.000000, T: 56700, Avg. loss: 69.672838\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 980.84, NNZs: 2304, Bias: 59.000000, T: 59850, Avg. loss: 61.677692\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1141.21, NNZs: 2304, Bias: -157.000000, T: 59850, Avg. loss: 57.991901\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 995.41, NNZs: 2304, Bias: 129.000000, T: 59850, Avg. loss: 55.654764\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1053.15, NNZs: 2304, Bias: -89.000000, T: 59850, Avg. loss: 57.205186\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1136.48, NNZs: 2304, Bias: -500.000000, T: 59850, Avg. loss: 60.897066\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1084.98, NNZs: 2304, Bias: 43.000000, T: 59850, Avg. loss: 56.394602\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1032.22, NNZs: 2304, Bias: -141.000000, T: 59850, Avg. loss: 70.426539\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1012.45, NNZs: 2304, Bias: 60.000000, T: 63000, Avg. loss: 63.002900\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1174.39, NNZs: 2304, Bias: -164.000000, T: 63000, Avg. loss: 55.638175\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1027.46, NNZs: 2304, Bias: 134.000000, T: 63000, Avg. loss: 55.738622\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1084.26, NNZs: 2304, Bias: -90.000000, T: 63000, Avg. loss: 56.006266\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1165.84, NNZs: 2304, Bias: -509.000000, T: 63000, Avg. loss: 58.621080\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1116.51, NNZs: 2304, Bias: 40.000000, T: 63000, Avg. loss: 57.223656\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1070.08, NNZs: 2304, Bias: -138.000000, T: 63000, Avg. loss: 69.146830\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1047.68, NNZs: 2304, Bias: 57.000000, T: 66150, Avg. loss: 59.367244Norm: 1208.06, NNZs: 2304, Bias: -164.000000, T: 66150, Avg. loss: 58.236465\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 22\n",
      "\n",
      "Total training time: 0.38 seconds.\n",
      "Norm: 1059.14, NNZs: 2304, Bias: 136.000000, T: 66150, Avg. loss: 60.544440\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 22\n",
      "-- Epoch 22\n",
      "Norm: 1116.59, NNZs: 2304, Bias: -92.000000, T: 66150, Avg. loss: 55.713770\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1195.57, NNZs: 2304, Bias: -525.000000, T: 66150, Avg. loss: 58.056402\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1152.46, NNZs: 2304, Bias: 42.000000, T: 66150, Avg. loss: 58.529753\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1101.51, NNZs: 2304, Bias: -143.000000, T: 66150, Avg. loss: 69.348911\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1083.19, NNZs: 2304, Bias: 65.000000, T: 69300, Avg. loss: 62.011426\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1238.20, NNZs: 2304, Bias: -170.000000, T: 69300, Avg. loss: 57.466063Norm: 1090.80, NNZs: 2304, Bias: 136.000000, T: 69300, Avg. loss: 55.891429\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 23\n",
      "\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1142.68, NNZs: 2304, Bias: -93.000000, T: 69300, Avg. loss: 56.390542\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1225.57, NNZs: 2304, Bias: -534.000000, T: 69300, Avg. loss: 52.949414\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1185.78, NNZs: 2304, Bias: 42.000000, T: 69300, Avg. loss: 57.506954\n",
      "Total training time: 0.41 seconds.\n",
      "Norm: 1132.88, NNZs: 2304, Bias: -144.000000, T: 69300, Avg. loss: 65.962297-- Epoch 23\n",
      "\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1274.38, NNZs: 2304, Bias: -173.000000, T: 72450, Avg. loss: 58.992244Norm: 1117.52, NNZs: 2304, Bias: 137.000000, T: 72450, Avg. loss: 56.762794\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1172.00, NNZs: 2304, Bias: -98.000000, T: 72450, Avg. loss: 53.349050\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 24\n",
      "\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1116.94, NNZs: 2304, Bias: 63.000000, T: 72450, Avg. loss: 63.256010\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1259.50, NNZs: 2304, Bias: -550.000000, T: 72450, Avg. loss: 56.102265\n",
      "Total training time: 0.42 seconds.Norm: 1167.46, NNZs: 2304, Bias: -153.000000, T: 72450, Avg. loss: 66.640781\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1217.31, NNZs: 2304, Bias: 42.000000, T: 72450, Avg. loss: 59.563623\n",
      "Total training time: 0.43 seconds.\n",
      "\n",
      "-- Epoch 24\n",
      "-- Epoch 24\n",
      "Norm: 1149.67, NNZs: 2304, Bias: 142.000000, T: 75600, Avg. loss: 57.233211\n",
      "Total training time: 0.43 seconds.\n",
      "Convergence after 24 epochs took 0.43 seconds\n",
      "Norm: 1200.82, NNZs: 2304, Bias: -107.000000, T: 75600, Avg. loss: 54.143561\n",
      "Total training time: 0.44 seconds.\n",
      "Norm: 1302.37, NNZs: 2304, Bias: -178.000000, T: 75600, Avg. loss: 55.490827\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 25\n",
      "-- Epoch 25\n",
      "Norm: 1151.54, NNZs: 2304, Bias: 66.000000, T: 75600, Avg. loss: 59.913743\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1286.74, NNZs: 2304, Bias: -557.000000, T: 75600, Avg. loss: 53.302318\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1246.65, NNZs: 2304, Bias: 39.000000, T: 75600, Avg. loss: 58.653765\n",
      "Total training time: 0.45 seconds.\n",
      "Convergence after 24 epochs took 0.45 seconds\n",
      "Norm: 1205.97, NNZs: 2304, Bias: -160.000000, T: 75600, Avg. loss: 67.718887\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1331.45, NNZs: 2304, Bias: -183.000000, T: 78750, Avg. loss: 53.318166\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1183.64, NNZs: 2304, Bias: 69.000000, T: 78750, Avg. loss: 58.690039\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1230.64, NNZs: 2304, Bias: -113.000000, T: 78750, Avg. loss: 52.511575\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1317.56, NNZs: 2304, Bias: -561.000000, T: 78750, Avg. loss: 54.824642\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1240.31, NNZs: 2304, Bias: -155.000000, T: 78750, Avg. loss: 64.648309\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1360.83, NNZs: 2304, Bias: -186.000000, T: 81900, Avg. loss: 54.150591\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1209.60, NNZs: 2304, Bias: 58.000000, T: 81900, Avg. loss: 62.074121Norm: 1256.60, NNZs: 2304, Bias: -117.000000, T: 81900, Avg. loss: 53.641754\n",
      "Total training time: 0.47 seconds.\n",
      "\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1353.17, NNZs: 2304, Bias: -573.000000, T: 81900, Avg. loss: 53.702998\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 27\n",
      "-- Epoch 27\n",
      "Norm: 1270.87, NNZs: 2304, Bias: -154.000000, T: 81900, Avg. loss: 66.928609\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1386.55, NNZs: 2304, Bias: -198.000000, T: 85050, Avg. loss: 55.583299\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1241.45, NNZs: 2304, Bias: 60.000000, T: 85050, Avg. loss: 59.560628\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1390.04, NNZs: 2304, Bias: -578.000000, T: 85050, Avg. loss: 52.946987\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1287.55, NNZs: 2304, Bias: -116.000000, T: 85050, Avg. loss: 51.859743\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1304.30, NNZs: 2304, Bias: -155.000000, T: 85050, Avg. loss: 67.017550\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1417.16, NNZs: 2304, Bias: -198.000000, T: 88200, Avg. loss: 57.530334\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1272.16, NNZs: 2304, Bias: 63.000000, T: 88200, Avg. loss: 58.859957\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1423.42, NNZs: 2304, Bias: -585.000000, T: 88200, Avg. loss: 53.364070\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1315.25, NNZs: 2304, Bias: -121.000000, T: 88200, Avg. loss: 50.187317\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1336.65, NNZs: 2304, Bias: -155.000000, T: 88200, Avg. loss: 63.500255\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1446.31, NNZs: 2304, Bias: -201.000000, T: 91350, Avg. loss: 52.966843\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1301.12, NNZs: 2304, Bias: 63.000000, T: 91350, Avg. loss: 54.152622\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1447.57, NNZs: 2304, Bias: -593.000000, T: 91350, Avg. loss: 52.986497Norm: 1340.95, NNZs: 2304, Bias: -126.000000, T: 91350, Avg. loss: 50.049752\n",
      "Total training time: 0.51 seconds.\n",
      "\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 30\n",
      "-- Epoch 30\n",
      "Norm: 1365.34, NNZs: 2304, Bias: -161.000000, T: 91350, Avg. loss: 64.378855\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1478.66, NNZs: 2304, Bias: -204.000000, T: 94500, Avg. loss: 52.758306\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1331.19, NNZs: 2304, Bias: 66.000000, T: 94500, Avg. loss: 55.315923\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1479.79, NNZs: 2304, Bias: -600.000000, T: 94500, Avg. loss: 51.585436\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1368.98, NNZs: 2304, Bias: -129.000000, T: 94500, Avg. loss: 50.857908\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1396.41, NNZs: 2304, Bias: -167.000000, T: 94500, Avg. loss: 63.209762\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1499.65, NNZs: 2304, Bias: -216.000000, T: 97650, Avg. loss: 51.252741\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1359.15, NNZs: 2304, Bias: 62.000000, T: 97650, Avg. loss: 55.447982\n",
      "Total training time: 0.54 seconds.\n",
      "Norm: 1511.43, NNZs: 2304, Bias: -608.000000, T: 97650, Avg. loss: 50.256012\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 32\n",
      "-- Epoch 32\n",
      "Norm: 1400.22, NNZs: 2304, Bias: -131.000000, T: 97650, Avg. loss: 52.653241\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1423.59, NNZs: 2304, Bias: -171.000000, T: 97650, Avg. loss: 61.035246\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1527.26, NNZs: 2304, Bias: -224.000000, T: 100800, Avg. loss: 51.914898\n",
      "Total training time: 0.56 seconds.\n",
      "Norm: 1540.45, NNZs: 2304, Bias: -617.000000, T: 100800, Avg. loss: 51.704775-- Epoch 33\n",
      "\n",
      "Total training time: 0.55 seconds.\n",
      "Norm: 1391.18, NNZs: 2304, Bias: 66.000000, T: 100800, Avg. loss: 57.599921\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 33\n",
      "-- Epoch 33\n",
      "Norm: 1430.63, NNZs: 2304, Bias: -128.000000, T: 100800, Avg. loss: 50.077502\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1451.18, NNZs: 2304, Bias: -175.000000, T: 100800, Avg. loss: 61.133883\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1554.12, NNZs: 2304, Bias: -226.000000, T: 103950, Avg. loss: 51.056057\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1571.13, NNZs: 2304, Bias: -627.000000, T: 103950, Avg. loss: 50.631887\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1418.07, NNZs: 2304, Bias: 66.000000, T: 103950, Avg. loss: 55.859974\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1460.71, NNZs: 2304, Bias: -135.000000, T: 103950, Avg. loss: 48.879638\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1481.63, NNZs: 2304, Bias: -175.000000, T: 103950, Avg. loss: 64.304721\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1578.31, NNZs: 2304, Bias: -234.000000, T: 107100, Avg. loss: 51.751299\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1601.69, NNZs: 2304, Bias: -633.000000, T: 107100, Avg. loss: 48.159452\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1447.78, NNZs: 2304, Bias: 72.000000, T: 107100, Avg. loss: 53.742164\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1479.34, NNZs: 2304, Bias: -136.000000, T: 107100, Avg. loss: 51.996676\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1512.26, NNZs: 2304, Bias: -178.000000, T: 107100, Avg. loss: 59.438318\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1601.25, NNZs: 2304, Bias: -240.000000, T: 110250, Avg. loss: 50.719225\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1625.81, NNZs: 2304, Bias: -643.000000, T: 110250, Avg. loss: 52.829963\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1474.42, NNZs: 2304, Bias: 66.000000, T: 110250, Avg. loss: 53.354947\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1540.89, NNZs: 2304, Bias: -173.000000, T: 110250, Avg. loss: 65.407086\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1500.46, NNZs: 2304, Bias: -146.000000, T: 110250, Avg. loss: 50.099325\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1631.14, NNZs: 2304, Bias: -240.000000, T: 113400, Avg. loss: 48.565429\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1653.06, NNZs: 2304, Bias: -653.000000, T: 113400, Avg. loss: 48.475149\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1501.92, NNZs: 2304, Bias: 59.000000, T: 113400, Avg. loss: 52.701593\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1570.78, NNZs: 2304, Bias: -168.000000, T: 113400, Avg. loss: 62.391386\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1522.06, NNZs: 2304, Bias: -146.000000, T: 113400, Avg. loss: 49.613598\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1654.64, NNZs: 2304, Bias: -238.000000, T: 116550, Avg. loss: 49.633561\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1678.34, NNZs: 2304, Bias: -653.000000, T: 116550, Avg. loss: 48.744988\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1529.99, NNZs: 2304, Bias: 60.000000, T: 116550, Avg. loss: 56.070459\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1594.64, NNZs: 2304, Bias: -166.000000, T: 116550, Avg. loss: 57.713770\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1683.93, NNZs: 2304, Bias: -236.000000, T: 119700, Avg. loss: 47.891990\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1551.81, NNZs: 2304, Bias: -153.000000, T: 116550, Avg. loss: 50.006561\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1703.51, NNZs: 2304, Bias: -664.000000, T: 119700, Avg. loss: 49.485696\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1554.58, NNZs: 2304, Bias: 66.000000, T: 119700, Avg. loss: 54.277881\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1621.89, NNZs: 2304, Bias: -170.000000, T: 119700, Avg. loss: 59.384777\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1706.10, NNZs: 2304, Bias: -238.000000, T: 122850, Avg. loss: 44.295674\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1577.47, NNZs: 2304, Bias: -156.000000, T: 119700, Avg. loss: 49.411598\n",
      "Total training time: 0.66 seconds.\n",
      "Convergence after 38 epochs took 0.66 seconds\n",
      "Norm: 1725.49, NNZs: 2304, Bias: -675.000000, T: 122850, Avg. loss: 46.840727\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1578.27, NNZs: 2304, Bias: 68.000000, T: 122850, Avg. loss: 53.362578\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1731.31, NNZs: 2304, Bias: -238.000000, T: 126000, Avg. loss: 47.677721\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1647.82, NNZs: 2304, Bias: -172.000000, T: 122850, Avg. loss: 59.749873\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1749.06, NNZs: 2304, Bias: -679.000000, T: 126000, Avg. loss: 46.557888\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1604.41, NNZs: 2304, Bias: 69.000000, T: 126000, Avg. loss: 54.124857\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1753.33, NNZs: 2304, Bias: -243.000000, T: 129150, Avg. loss: 47.925262\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1671.67, NNZs: 2304, Bias: -181.000000, T: 126000, Avg. loss: 58.891688\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1772.69, NNZs: 2304, Bias: -682.000000, T: 129150, Avg. loss: 47.704290\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1630.70, NNZs: 2304, Bias: 72.000000, T: 129150, Avg. loss: 51.543211\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1781.17, NNZs: 2304, Bias: -240.000000, T: 132300, Avg. loss: 50.237382\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1696.40, NNZs: 2304, Bias: -182.000000, T: 129150, Avg. loss: 55.672083\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1797.98, NNZs: 2304, Bias: -686.000000, T: 132300, Avg. loss: 49.882752\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1657.39, NNZs: 2304, Bias: 75.000000, T: 132300, Avg. loss: 51.057666\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1802.59, NNZs: 2304, Bias: -242.000000, T: 135450, Avg. loss: 44.414660\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1723.87, NNZs: 2304, Bias: -177.000000, T: 132300, Avg. loss: 59.995848\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1823.16, NNZs: 2304, Bias: -697.000000, T: 135450, Avg. loss: 44.516513\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1684.30, NNZs: 2304, Bias: 74.000000, T: 135450, Avg. loss: 50.808675\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1823.64, NNZs: 2304, Bias: -242.000000, T: 138600, Avg. loss: 45.943869\n",
      "Total training time: 0.74 seconds.\n",
      "Convergence after 44 epochs took 0.74 seconds\n",
      "Norm: 1752.52, NNZs: 2304, Bias: -179.000000, T: 135450, Avg. loss: 59.932473Norm: 1853.91, NNZs: 2304, Bias: -704.000000, T: 138600, Avg. loss: 49.435293\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 45\n",
      "\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1710.72, NNZs: 2304, Bias: 77.000000, T: 138600, Avg. loss: 49.634380\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1879.26, NNZs: 2304, Bias: -709.000000, T: 141750, Avg. loss: 48.438425\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1777.16, NNZs: 2304, Bias: -179.000000, T: 138600, Avg. loss: 59.749957\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1732.69, NNZs: 2304, Bias: 76.000000, T: 141750, Avg. loss: 52.142448\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1902.14, NNZs: 2304, Bias: -718.000000, T: 144900, Avg. loss: 46.920277\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1799.71, NNZs: 2304, Bias: -184.000000, T: 141750, Avg. loss: 59.239578\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1756.73, NNZs: 2304, Bias: 84.000000, T: 144900, Avg. loss: 50.585471\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1925.82, NNZs: 2304, Bias: -726.000000, T: 148050, Avg. loss: 47.689111\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1826.70, NNZs: 2304, Bias: -179.000000, T: 144900, Avg. loss: 57.576174\n",
      "Total training time: 0.78 seconds.\n",
      "Convergence after 46 epochs took 0.78 seconds\n",
      "Norm: 1949.31, NNZs: 2304, Bias: -734.000000, T: 151200, Avg. loss: 43.544421Norm: 1780.71, NNZs: 2304, Bias: 83.000000, T: 148050, Avg. loss: 48.828476\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 49\n",
      "\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1973.85, NNZs: 2304, Bias: -738.000000, T: 154350, Avg. loss: 45.039483\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1806.73, NNZs: 2304, Bias: 83.000000, T: 151200, Avg. loss: 49.469258\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1999.21, NNZs: 2304, Bias: -743.000000, T: 157500, Avg. loss: 44.396295\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 1831.82, NNZs: 2304, Bias: 87.000000, T: 154350, Avg. loss: 49.118149\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 2015.51, NNZs: 2304, Bias: -751.000000, T: 160650, Avg. loss: 44.238231\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 1853.78, NNZs: 2304, Bias: 81.000000, T: 157500, Avg. loss: 49.785464\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 2036.68, NNZs: 2304, Bias: -753.000000, T: 163800, Avg. loss: 46.299908\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 1877.42, NNZs: 2304, Bias: 81.000000, T: 160650, Avg. loss: 50.479475\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 2059.41, NNZs: 2304, Bias: -753.000000, T: 166950, Avg. loss: 44.779562\n",
      "Total training time: 0.85 seconds.\n",
      "Convergence after 53 epochs took 0.85 seconds\n",
      "Norm: 1899.75, NNZs: 2304, Bias: 79.000000, T: 163800, Avg. loss: 49.420889\n",
      "Total training time: 0.85 seconds.\n",
      "Convergence after 52 epochs took 0.85 seconds\n",
      "0.22285714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   7 out of   7 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "ev = Evaluation(fer, 500)\n",
    "model = Perceptron(tol=1e-3, random_state=0, verbose=1, n_jobs=8)\n",
    "ev.testModel(10, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
